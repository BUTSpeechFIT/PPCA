{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Principal Components Analysis (PPCA)\n",
    "\n",
    "This notebook illustrate how to build and train a PPCA model using the [PPCA julia package](). More precisely, the package implements the [\"Variational Principal Components Analysis\"](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-vpca-icann-99.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../\");\n",
    "\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using ExpFamilyDistributions\n",
    "using PPCA\n",
    "\n",
    "using BasicDataLoaders\n",
    "\n",
    "# Plotting function\n",
    "include(\"plotting.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synethic data\n",
    "\n",
    "Our toy data is simply samples from a 2-dimensional Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Float64\n",
    "Œº = T[1, 1] \n",
    "Œ£ = T[2. -.7; -.7  0.3]\n",
    "\n",
    "function draw(n, Œº, Œ£)\n",
    "    L = cholesky(Œ£).L\n",
    "    [Œº + L*randn(T, 2) for i in 1:n] \n",
    "end\n",
    "\n",
    "X = draw(30, Œº, Œ£)\n",
    "dl = DataLoader(X, batchsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = PPCAModel(T, datadim = 2, latentdim = 2, pstrength = 1e-3)\n",
    "\n",
    "# Initialize the variational posteriors\n",
    "Œ∏posts = Œ∏posteriors(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The code below is only for plotting and it is not needed to use the model in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for the training\n",
    "epochs = 30\n",
    "\n",
    "# For visualisation\n",
    "anim = Animation()\n",
    "\n",
    "# Training\n",
    "elbos = []\n",
    "function monitor(epoch)\n",
    "    e = elbo(model, dl, Œ∏posts) \n",
    "    ND = length(X) * length(X[1])\n",
    "    push!(elbos, e / ND)\n",
    "    \n",
    "    l = @layout [a b; c d]\n",
    "    \n",
    "    p1 = plot(legend = false, ylims = (-1,3), xlims = (-3, 5), title = \"Observation space\")\n",
    "    scatter!(p1, getindex.(X, 1), getindex.(X, 2), mark = :cross, color = :blue)\n",
    "    plotmodel!(p1, model, Œ∏posts)\n",
    "\n",
    "    p3 = plot(xlabel = \"epoch\", xlims = (1, epochs), ylims = (-2, -1),\n",
    "              color = :blue, legend = :bottomleft)\n",
    "    plot!(p3, elbos,linewidth = 2, label = \"ELBO\")\n",
    "\n",
    "    W = hcat([p.Œº for p in Œ∏posts[:w]]...)\n",
    "    w‚ÇÅ = W[1:end-1,:][1, :]\n",
    "    w‚ÇÇ = W[1:end-1,:][2, :]\n",
    "    p4 = bar([\"||w‚ÇÅ||¬≤\", \"||w‚ÇÇ||¬≤\"], [norm(w‚ÇÅ), norm(w‚ÇÇ)],  ylims = (0,3),\n",
    "             legend = false, color = :black, title = \"norm. bases\")\n",
    "\n",
    "    hposts = hposteriors(model, X, Œ∏posts)\n",
    "    p2 = plot(legend = false, title = \"Latent space\", xlims = (-3, 3), ylims = (-3, 3))\n",
    "    plotnormal2d!(p2, model.hprior.Œº, model.hprior.Œ£, color = :blue, linewidth = 2)\n",
    "    plotnormals!(p2, hposts)\n",
    "\n",
    "    plot(p1, p2, p3, p4, layout = l, size = (800, 500))\n",
    "    \n",
    "    frame(anim)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual training is done just by calling the function `fit!`. Note that `callback` is an optional argument and it is only for the user to monitor the convergence of the model. In this example, our callback is the `monitor` function which:\n",
    "  * compute the ELBO of the data and the model\n",
    "  * do some plotting  \n",
    "  \n",
    "This is of course time consuming and should be avoided in real cases. We recommend to use a simpler callback (if you want to use one) which computes the ELBO on a subset of the data every $K$ epochs. For instance, the callback can look like this:\n",
    "```julia\n",
    "function simplemonitor(epoch)\n",
    "    # Compute the elbo every 5 epochs\n",
    "    if epoch % 5 == 0 \n",
    "        ùìõ = elbo(model, dl, Œ∏posts) \n",
    "        println(\"epoch = $e  ùìõ = $ùìõ\")\n",
    "    end\n",
    "end\n",
    "\n",
    "fit!(model, dl, Œ∏posts, epochs = epochs, callback = simplemonitor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor(0) # Just to add the initialization of the model to the animation\n",
    "\n",
    "# Training\n",
    "# Internally, this function uses functionality of the\n",
    "# Distributed package. If you want to accelerate the \n",
    "# training, call `addprocs(...)` to create more workers.\n",
    "fit!(model, dl, Œ∏posts, epochs = epochs, callback = monitor)\n",
    "\n",
    "gif(anim, \"demo.gif\", fps=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
